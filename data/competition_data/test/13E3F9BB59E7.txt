I think Facial Action cording system should not be able to identify human emotions. As in pargraph 1 it says, "She's 83 percent happy, 9 percent disgusted,6 percent,and 2 percent angry". What if those percentages are wrong. Those percentages could not be real just by looking at the image she could of been happy. Those emotions are just identified by a computer, not all technology could be right. The computers/technology just identify but could be wrong by looking at the face of the image/picture they have used to dectect the emotion.

As in pargraph 3 it says, "The process begins when the computer constucts a 3-D computer model of the face". Ther computer cops the face of the image and copys and make it 3-D. The expression is compared against a neutral face. The motion that is shown is showing no emotion at all which. The computer that they use shows wether the face is happy or sad.

The computers use calculation to show wether the face of the image is happy or sad. There is certain muscles in every face that make the bonestructor make it what you look like. There are six emotions that are used every day sad,mad,happy,scared,disgust,and susprised. I think computers/technology should not be able to dectect the emotion just by what the picture/image looks like. There muscles that move in your body every day that is called an "Action unit" as it says in pargraph 3.      